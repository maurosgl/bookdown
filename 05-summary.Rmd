# **Prophet**


```{r, cargue_datos, echo = FALSE}
csv_file <- "C:/Users/Steba/OneDrive/Escritorio/kalimati_tarkari_dataset (2).csv"

```




```{r, lectura_datos, echo = FALSE}
# Leer datos
data_raw <- readr::read_csv(csv_file, show_col_types = FALSE)

# Estandarizar nombres a snake para facilitar mapeo
nms <- tolower(gsub("[^a-zA-Z0-9]+", "_", names(data_raw)))

# Intentar mapear columnas canonicas (Commodity / Date / Minimum / Maximum / Average)
df <- data_raw
names(df) <- nms

# Mapeo flexible (case-insensitive)
pick_first <- function(cands) {
hit <- intersect(cands, names(df))
if (length(hit) == 0) return(NA_character_) else return(hit[1])
}

col_commodity <- pick_first(c("commodity","item","product","variety","name"))
col_date      <- pick_first(c("date","fecha","day"))
col_min       <- pick_first(c("minimum","min","min_price","price_min"))
col_max       <- pick_first(c("maximum","max","max_price","price_max"))
col_avg       <- pick_first(c("average","avg","avg_price","price_avg","mean_price"))

req <- c(col_commodity, col_date, col_min, col_max, col_avg)

if (any(is.na(req))) {
stop("No fue posible mapear columnas clave (commodity/date/min/max/average). Revisa nombres del CSV: ",
paste(names(df), collapse = ", "))
}


```

```{r, librerias, echo = FALSE}
# --- Cargar librer√≠as necesarias ---
libs <- c("dplyr", "tidyr", "lubridate", "stringr", "tseries")
for (pkg in libs) {
  if (!require(pkg, character.only = TRUE)) {
    install.packages(pkg, dependencies = TRUE)
    library(pkg, character.only = TRUE)
  }
}

# Asegurar el operador %>% disponible
if (!exists("%>%")) {
  library(magrittr)
}

```


```{r, transformacion, echo = FALSE}
data <- df %>%
transmute(
Commodity = .data[[col_commodity]],
Date      = as.Date(.data[[col_date]]),
Unit      = NA_character_,     
Minimum   = as.numeric(.data[[col_min]]),
Maximum   = as.numeric(.data[[col_max]]),
Average   = as.numeric(.data[[col_avg]])
)

# Chequeos basicos
stopifnot(inherits(data$Date, "Date"))
```


```{r, v_previa, echo = FALSE}
# Vista previa y estructura
head(data)
str(data)
summary(data)
```


```{r, data_na_dup, echo = FALSE}
# NAs y duplicados generales
sum(is.na(data))
sum(duplicated(data))
```


```{r, echo = FALSE}
if (!require(ggplot2)) {
  install.packages("ggplot2")
  library(ggplot2)
}
```


```{r, echo = FALSE}
#analisis univariado
ggplot(data, aes(x = Average)) +
geom_histogram(binwidth = 5, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Distribucion de Precios Promedio", x = "Precio Promedio", y = "Frecuencia") +
theme_minimal()

```

```{r, echo = FALSE}
data %>%
summarise(
Mean = mean(Average, na.rm = TRUE),
Median = median(Average, na.rm = TRUE),
SD = sd(Average, na.rm = TRUE),
Min = min(Average, na.rm = TRUE),
Max = max(Average, na.rm = TRUE)
)

```

```{r, echo = FALSE}
#analisis bivariado

ggplot(data, aes(x = Minimum, y = Maximum)) +
geom_point(alpha = 0.5) +
labs(title = "Relacion entre Precio Minimo y Maximo", x = "Precio Minimo", y = "Precio Maximo") +
theme_minimal()

```

```{r, echo = FALSE}
cor(data$Minimum, data$Maximum, use = "complete.obs")

```


```{r, echo = FALSE}
if ("Potato Red" %in% unique(data$Commodity)) {
target_item <- "Potato Red"
} else {
target_item <- data %>%
count(Commodity, sort = TRUE) %>%
slice(1) %>%
pull(Commodity)
}

target_item

```


```{r, echo = FALSE}
# Filtrar y regularizar serie diaria

potatored <- data %>%
filter(Commodity == target_item) %>%
select(Date, Average) %>%
group_by(Date) %>%
summarise(Average = mean(Average), .groups = "drop") %>%
complete(Date = seq(min(Date, na.rm = TRUE), max(Date, na.rm = TRUE), by = "day")) %>%
arrange(Date)

# Construir objeto ts (diario, 365)

pot_ts <- ts(
potatored$Average,
start = c(lubridate::year(min(potatored$Date, na.rm = TRUE)),
lubridate::yday(min(potatored$Date, na.rm = TRUE))),
frequency = 365
)

```


```{r, echo = FALSE}
# --- Hotfix para error "rstudio$.rs.isDesktop()" ---
if (exists("rstudio", envir = .GlobalEnv)) rm("rstudio", envir = .GlobalEnv)
options(device.ask.default = FALSE)

if (capabilities("cairo")) {
  knitr::opts_chunk$set(dev = "png", dpi = 150, dev.args = list(type = "cairo"))
  options(bitmapType = "cairo")
} else {
  knitr::opts_chunk$set(dev = "png", dpi = 150)
}

try(graphics.off(), silent = TRUE)

```

```{r, echo = FALSE}
library(ggplot2)
library(forecast)

autoplot(pot_ts) +
  labs(
    title = "üìà Serie de tiempo: precio promedio",
    y = "Precio promedio",
    x = "Tiempo"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(face = "bold", color = "#2C3E50", size = 15),
    axis.title = element_text(face = "bold", color = "#34495E"),
    panel.grid.minor = element_blank(),
    panel.grid.major = element_line(color = "grey90")
  )

```


```{r, echo = FALSE}
ggAcf(pot_ts) + labs(title = "ACF del precio promedio (diario)")

```



```{r, echo = FALSE}
if (exists("rstudio", envir = .GlobalEnv)) rm("rstudio", envir = .GlobalEnv)
options(device.ask.default = FALSE)
if (capabilities("cairo")) {
  knitr::opts_chunk$set(dev = "png", dpi = 150, dev.args = list(type = "cairo"))
  options(bitmapType = "cairo")
} else {
  knitr::opts_chunk$set(dev = "png", dpi = 150)
}

```

```{r, echo = FALSE}
# Hotfix global para gr√°ficos (evita el viewer de RStudio)
if (exists("rstudio", envir = .GlobalEnv)) rm("rstudio", envir = .GlobalEnv)
options(device.ask.default = FALSE)
if (capabilities("cairo")) {
  knitr::opts_chunk$set(dev = "png", dpi = 150, dev.args = list(type = "cairo"))
  options(bitmapType = "cairo")
} else {
  knitr::opts_chunk$set(dev = "png", dpi = 150)
}

```


```{r, echo = FALSE}
# Hotfix global para gr√°ficos (evita el viewer de RStudio)
if (exists("rstudio", envir = .GlobalEnv)) rm("rstudio", envir = .GlobalEnv)
options(device.ask.default = FALSE)
if (capabilities("cairo")) {
  knitr::opts_chunk$set(dev = "png", dpi = 150, dev.args = list(type = "cairo"))
  options(bitmapType = "cairo")
} else {
  knitr::opts_chunk$set(dev = "png", dpi = 150)
}
```

```{r, echo = FALSE}
# --- Hotfix global: NO viewer, render a PNG ---
if (exists("rstudio", envir = .GlobalEnv)) rm("rstudio", envir = .GlobalEnv)
options(device.ask.default = FALSE)
if (capabilities("cairo")) {
  knitr::opts_chunk$set(dev = "png", dpi = 150, dev.args = list(type = "cairo"))
  options(bitmapType = "cairo")
} else {
  knitr::opts_chunk$set(dev = "png", dpi = 150)
}

# Paquetes m√≠nimos
libs <- c("dplyr","tidyr","lubridate","slider")
for (p in libs) if (!require(p, character.only=TRUE)) {install.packages(p); library(p, character.only=TRUE)}

# Utilidad para guardar junto al Rmd
safe_dir <- function(){
  f <- tryCatch(knitr::current_input(), error=function(e) NULL)
  if (!is.null(f) && nzchar(f)) dirname(normalizePath(f)) else getwd()
}


```



```{r, echo = FALSE}
# ...
fig_path <- "figuras/stl2.png"
# ...
message("‚ö†Ô∏è La imagen 'stl2.png' a√∫n no existe...")

```



```{r, serie_basica_png, fig.show = 'hide'}
# Crear carpeta de figuras si no existe
if (!dir.exists("figuras")) dir.create("figuras", recursive = TRUE)

# Definir ruta de salida del gr√°fico
outfile <- file.path("figuras", "serie_basica.png")

# Generar y guardar el gr√°fico
if (capabilities("cairo")) {
  png(outfile, width = 1400, height = 600, res = 150, type = "cairo")
} else {
  png(outfile, width = 1400, height = 600, res = 150)
}

op <- par(mar = c(4, 4, 3, 1), mgp = c(2.2, 0.8, 0))
plot(
  potatored$Date, potatored$Average, type = "l",
  col = "#1F77B4", lwd = 1.2,
  xlab = "Fecha", ylab = "Precio promedio",
  main = "Serie de tiempo: precio promedio"
)
par(op)
dev.off()

# Mostrar la imagen generada (m√©todo estandarizado)
knitr::include_graphics(outfile)

```


```{r, stl_png, fig.show = 'hide'}
# Asegurar que la carpeta 'figuras' exista (relativa al proyecto)
if (!dir.exists("figuras")) dir.create("figuras", recursive = TRUE)

# Definir la ruta de salida est√°ndar
outfile <- file.path("figuras", "stl.png")

# --- L√≥gica de c√°lculo STL (existente) ---
y <- ts(potatored$Average, frequency = 365)
# interp lineal base (necesaria para STL si hay NAs)
y <- approx(seq_along(y), y, xout=seq_along(y))$y 
fit <- stl(ts(y, frequency=365), s.window="periodic", robust=TRUE)
# --- Fin l√≥gica de c√°lculo ---

# Generar y guardar el gr√°fico
if (capabilities("cairo")) {
  png(outfile, width=1400, height=900, res=150, type="cairo")
} else {
  png(outfile, width=1400, height=900, res=150)
}
op <- par(mfrow=c(4,1), mar=c(3,4,2,1))
plot(fit$time.series[,"remainder"], type="l", col="#7D3C98", main="Residuo", xlab="", ylab="")
plot(fit$time.series[,"seasonal"],  type="l", col="#117A65", main="Estacionalidad", xlab="", ylab="")
plot(fit$time.series[,"trend"],      type="l", col="#E67E22", main="Tendencia", xlab="", ylab="")
plot(y, type="l", col="#1F77B4", main="Serie (interpolada)", xlab="Tiempo", ylab="")
par(op); dev.off()

# Incluir la imagen guardada en el bookdown
# Esta l√≠nea reemplaza el 'cat(sprintf(...))'
knitr::include_graphics(outfile)
```

```{r, echo = FALSE}
# --- Promedios m√≥viles (gr√°fico generado autom√°ticamente) ---

if (!dir.exists("figuras")) dir.create("figuras", recursive = TRUE)

fig_path <- "figuras/promedios_moviles.png"

# Generar el gr√°fico y guardarlo

png(fig_path, width = 1200, height = 600, res = 150)
plot(potatored$Average, type = "l", col = "steelblue",
main = "Promedios m√≥viles (7 y 30 d√≠as)",
ylab = "Precio promedio", xlab = "Tiempo")
lines(stats::filter(potatored$Average, rep(1/7,7)), col = "red", lwd = 2)
lines(stats::filter(potatored$Average, rep(1/30,30)), col = "green", lwd = 2)
legend("topright", legend = c("Original", "7 d√≠as", "30 d√≠as"),
col = c("steelblue", "red", "green"), lty = 1, lwd = 2)
dev.off()

# Mostrar la imagen

knitr::include_graphics(fig_path)
```


```{r, echo = FALSE}
pot_lags <- potatored %>%
  mutate(
    lag1  = dplyr::lag(Average, 1),
    lag7  = dplyr::lag(Average, 7),
    lag30 = dplyr::lag(Average, 30)
  )

# Scatter y_t vs y_{t-1}
ggplot(pot_lags, aes(lag1, Average)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.7) +
  theme_minimal() +
  labs(title = "Scatter rezago 1 (y_t vs y_{t-1})", x = "y_{t-1}", y = "y_t")

# Scatter y_t vs y_{t-7}
ggplot(pot_lags, aes(lag7, Average)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.7) +
  theme_minimal() +
  labs(title = "Scatter rezago 7 (aprox. semanal)", x = "y_{t-7}", y = "y_t")



# ACF/PACF (serie regularizada)
ggAcf(pot_ts)  + labs(title = "ACF precio promedio (diario)")
ggPacf(pot_ts) + labs(title = "PACF precio promedio (diario)")

```


```{r, echo = FALSE}
fit_stl <- stl(na.interp(pot_ts), s.window = "periodic", robust = TRUE)
autoplot(fit_stl) + labs(title = "STL precio promedio")

```

```{r, echo = FALSE}
# Usaremos una version "limpia" de la serie via interpolacion lineal base R, partimos de 'potatored' (data.frame terminado) y/o de 'pot_ts' (ts original)

y <- as.numeric(pot_ts)

if (anyNA(y)) {
  idx_ok <- which(!is.na(y))
  y_interp <- approx(x = idx_ok, y = y[idx_ok], xout = seq_along(y))$y
} else {
  y_interp <- y}


pot_ts_clean <- ts(
  y_interp,
  start = start(pot_ts),
  frequency = frequency(pot_ts)
)


```



```{r, echo = FALSE}
# Prueba de ra√≠z unitaria de Dickey-Fuller aumentada

adf_result <- adf.test(pot_ts_clean)
adf_result

```
```{r, echo = FALSE}
# al tener en el ADF inicial: p = 0.1301 ‚Üí se concluye que es no estacionaria al nive, por lo cual procedemos con transformacion y diferenciacion en escalones


# Serie base a usar en esta etapa y verificamos que no tenga na ni valores negativos
y0 <- pot_ts_clean 
sum(is.na(y0))
all(y0>0)

```
```{r, echo = FALSE}

y_log <- log(y0)
range(y0, na.rm = TRUE); range(y_log, na.rm = TRUE)  # solo para verificar el cambio de escala
adf_log <- adf.test(y_log)
adf_log$p.value


```


```{r, echo = FALSE}


y_diff1 <- diff(y_log, differences = 1)
adf_diff1 <- tseries::adf.test(na.omit(y_diff1))
adf_diff1$p.value
```


```{r, echo = FALSE}
y_hw <- na.interp(pot_ts) 
freq <- frequency(y_hw)   
length(y_hw); freq
```



```{r, echo = FALSE}

h_test <- min(365, floor(length(y_hw)*0.2))  # 1 a√±o o ~20% si no alcanza
n <- length(y_hw)
y_train <- window(y_hw, end = time(y_hw)[n - h_test])
y_test  <- window(y_hw, start = time(y_hw)[n - h_test + 1])

autoplot(y_hw) +
  geom_vline(xintercept = time(y_hw)[n - h_test + 1], linetype = 2) +
  labs(title = "Serie completa con corte train/test",
       y = "Precio promedio", x = "Tiempo") +
  theme_minimal()
```


```{r, echo = FALSE}
library(zoo)

# y_hw: serie ts diaria (freq=365)
stopifnot(frequency(y_hw) %in% c(365, 366))

#secuencia de fechas real para la ts diaria
tsp_hw <- tsp(y_hw)                     # c(start, end, freq)
start_year <- floor(tsp_hw[1])
start_frac <- tsp_hw[1] - start_year
start_date <- as.Date(paste0(start_year, "-01-01")) + round(start_frac * 365.25)

fechas_all <- seq.Date(from = start_date, by = "day", length.out = length(y_hw))
z_all <- zoo(as.numeric(y_hw), fechas_all)

#Agregar por semana calendario
z_week <- aggregate(z_all, as.Date(cut(index(z_all), "week")), mean, na.rm = TRUE)

#Convertimos a ts semanal (freq = 52)
y_w <- ts(as.numeric(z_week), frequency = 52)

#Split train/test semanal
h_test_w <- min(52, floor(length(y_w) * 0.2))
n_w <- length(y_w)
y_train_w <- window(y_w, end = time(y_w)[n_w - h_test_w])
y_test_w  <- window(y_w, start = time(y_w)[n_w - h_test_w + 1])

autoplot(y_w) +
  geom_vline(xintercept = time(y_w)[n_w - h_test_w + 1], linetype = 2) +
  labs(title = "Serie semanal con corte train/test",
       y = "Precio promedio semanal", x = "Tiempo") +
  theme_minimal()

```


```{r, echo = FALSE}
# Horizonte de prueba
h <- length(y_test_w)

# Modelos de suavizamiento base
# Suavizamiento exponencial simple (SES)
fit_ses_w <- ets(y_train_w, model = "ANN")
fc_ses_w  <- forecast(fit_ses_w, h = h)

# Holt: tendencia aditiva
fit_holt_w <- ets(y_train_w, model = "AAN")
fc_holt_w  <- forecast(fit_holt_w, h = h)

# Holt-Winters con funci√≥n base stats::HoltWinters
fit_hwa_w <- HoltWinters(y_train_w, seasonal = "additive")
fit_hwm_w <- HoltWinters(y_train_w, seasonal = "multiplicative")

# Generar pron√≥sticos a h pasos
fc_hwa_w <- forecast(fit_hwa_w, h = h)
fc_hwm_w <- forecast(fit_hwm_w, h = h)

# Resumen de par√°metros de suavizamiento
pars_w <- list(
  SES      = fit_ses_w$par,
  Holt     = fit_holt_w$par,
  HW_adi   = fit_hwa_w$coefficients,
  HW_multi = fit_hwm_w$coefficients
)

metodos_w <- c(
  SES    = fit_ses_w$method,
  Holt   = fit_holt_w$method,
  HW_adi = "Holt-Winters aditivo (HoltWinters)",
  HW_mul = "Holt-Winters multiplicativo (HoltWinters)"
)

list(pars = pars_w, metodos = metodos_w)

```


```{r, echo = FALSE}

# Calcular m√©tricas de error para cada modelo en el conjunto de prueba
acc_tbl <- bind_rows(
  data.frame(model = "SES",    accuracy(fc_ses_w,  y_test_w)),
  data.frame(model = "Holt",   accuracy(fc_holt_w, y_test_w)),
  data.frame(model = "HW_adi", accuracy(fc_hwa_w,  y_test_w)),
  data.frame(model = "HW_mul", accuracy(fc_hwm_w,  y_test_w))
) %>%
  select(model, RMSE, MAE, MAPE)

acc_tbl

```


```{r, echo = FALSE}
best_fc <- fc_hwm_w  # ganador: Holt-Winters multiplicativo
checkresiduals(best_fc$model)

```


```{r, echo = FALSE}

# Reentrenar el modelo Holt-Winters multiplicativo con toda la serie semanal
fit_final_hw <- HoltWinters(y_w, seasonal = "multiplicative")

# pron√≥stico a 52 semanas (‚âà 1 a√±o)
h_future <- 52
fc_final_hw <- forecast(fit_final_hw, h = h_future)

# Visualizacion del pron√≥stico final
autoplot(fc_final_hw) +
  labs(
    title = "Pronostico final Holt-Winters multiplicativo (52 semanas)",
    y = "Precio promedio semanal",
    x = "Tiempo"
  ) +
  theme_minimal()

# primeras filas de la tabla de pronostico
head(data.frame(
  Semana = time(fc_final_hw$mean),
  Pronostico = round(as.numeric(fc_final_hw$mean), 2),
  LI_80 = round(fc_final_hw$lower[,1], 2),
  LS_80 = round(fc_final_hw$upper[,1], 2),
  LI_95 = round(fc_final_hw$lower[,2], 2),
  LS_95 = round(fc_final_hw$upper[,2], 2)
), 10)

```




```{r, echo = FALSE}
# Serie semanal en log
y_w_log      <- log(y_w)
y_train_w_log <- log(y_train_w)
y_test_w_log  <- log(y_test_w)

# Horizonte
h <- length(y_test_w)
lengths <- c(total = length(y_w_log), train = length(y_train_w_log), test = length(y_test_w))
lengths
```
```{r, echo = FALSE}
ggAcf(y_train_w_log)  + labs(title = "ACF log semanal (train)")
ggPacf(y_train_w_log) + labs(title = "PACF log semanal (train)")
```
```{r, echo = FALSE}
# Candidato 1: ARIMA no estacional
fit_ns <- auto.arima(
  y_train_w_log,
  seasonal      = FALSE,
  stepwise      = TRUE,
  approximation = TRUE,
  allowmean     = TRUE,
  allowdrift    = TRUE
)

# Candidato 2:

m <- frequency(y_train_w_log)  
D_est <- nsdiffs(y_train_w_log)

y_train_w_log_sdiff <- diff(y_train_w_log, lag = m, differences = D_est)

fit_seas_fast <- auto.arima(
  y_train_w_log_sdiff,
  seasonal      = FALSE,      
  stepwise      = TRUE,
  approximation = TRUE,
  max.p         = 5,
  max.q         = 5,
  max.order     = 8,
  allowmean     = TRUE,
  allowdrift    = TRUE
)

fit_seas_fast

# Extraer orden no estacional del modelo rapido sobre la serie s-diferenciada
ord <- arimaorder(fit_seas_fast)        
p <- ord["p"]; d <- 0; q <- ord["q"]

# Periodo estacional y orden estacional detectado
m     <- frequency(y_train_w_log)
D_est <- nsdiffs(y_train_w_log) 

#  Ajuste SARIMA equivalente sobre la serie en log SIN diferenciar manualmente
#    SARIMA(p,d,q)(P=0, D=D_est, Q=0)[m], sin media (zero mean) como indico el modelo rapido
fit_seas_refit <- Arima(
  y_train_w_log,
  order    = c(p, d, q),
  seasonal = list(order = c(0, D_est, 0), period = m),
  include.mean = FALSE
)

fit_seas_refit

```



```{r, echo = FALSE}
# 1) Pronostico en log y transformacion a escala original
h <- length(y_test_w)
fc_seas_log <- forecast(fit_seas_refit, h = h)

fc_seas <- fc_seas_log
fc_seas$mean  <- exp(fc_seas_log$mean)
fc_seas$lower <- exp(fc_seas_log$lower)
fc_seas$upper <- exp(fc_seas_log$upper)

# 2) Comparacion
tiene_ns <- exists("fc_ns")

if (tiene_ns) {
  acc_tbl_arima <- bind_rows(
    data.frame(model = "ARIMA_ns",  accuracy(fc_ns,   y_test_w)),
    data.frame(model = "SARIMA_52", accuracy(fc_seas, y_test_w))
  ) %>% select(model, RMSE, MAE, MAPE)
} else {
  acc_tbl_arima <- bind_rows(
    data.frame(model = "SARIMA_52", accuracy(fc_seas, y_test_w))
  ) %>% select(model, RMSE, MAE, MAPE)
}

acc_tbl_arima
```



```{r, echo = FALSE}
# Evaluacion de residuos del modelo SARIMA
checkresiduals(fit_seas_refit)
```



```{r, echo = FALSE}
# Pronostico a 52 semanas
fc_final <- forecast(fit_seas_refit, h = 52)

# Volver a escala original
fc_final_exp <- fc_final
fc_final_exp$mean  <- exp(fc_final$mean)
fc_final_exp$lower <- exp(fc_final$lower)
fc_final_exp$upper <- exp(fc_final$upper)

fc_final_exp$x <- exp(fc_final_exp$x)
# ---------------------------------

# OPCION A: Grafico simple del pronostico (AHORA SER√Å COHERENTE)
autoplot(fc_final_exp) +
  labs(
    title    = "Pronostico final SARIMA(1,0,0)(0,1,0)[52]",
    subtitle = "Proyeccion de precios semanales (Escala Original)",
    x = "Tiempo", y = "Precio promedio semanal"
  ) +
  theme_minimal()

```


```{r, echo = FALSE}
# Ajuste autom√°tico con comparaci√≥n AIC/BIC

y <- ts(potatored$Average, frequency = 7)
fit_arima <- forecast::auto.arima(
y, seasonal = TRUE, stepwise = FALSE, approximation = FALSE
)
fit_arima
AIC(fit_arima); BIC(fit_arima)
```


```{r, echo = FALSE}
h <- 30
fc <- forecast::forecast(fit_arima, h = h)
fc

```



Se cargan las librer√≠as fpp3 y fable.prophet. Luego, se preparan los datos agregando la serie diaria a un formato tsibble semanal

```{r}

library(fpp3)
library(fable.prophet)

pot_weekly_tsbl <- potatored %>%
  mutate(Week = yearweek(Date)) %>%     
  group_by(Week) %>%
  summarise(
    Precio = mean(Average, na.rm = TRUE),       
    .groups = "drop"
  ) %>%
  as_tsibble(index = Week)

# Vista rapida de la serie semanal
pot_weekly_tsbl
```

## **Ajustamos el modelo Prophet**

Definimos expl√≠citamente una estacionalidad anual (season("year")) y semanal (season("week")) mediante series de Fourier (con √≥rdenes 10 y 3, respectivamente) para capturar los patrones c√≠clicos.


```{r}
fit_prophet <- pot_weekly_tsbl %>%
  model(
    prophet = prophet(
      Precio ~ season("year", order = 10) +   #anual
                 season("week", order = 3)    #semanal
    )
  )

fit_prophet
```

## **Generaci√≥n del modelo Prophet**

Generamos un pron√≥stico a 52 semanas (un a√±o) usando el modelo Prophet ajustado. Procedemos a graficar este pron√≥stico (fc_prophet) junto a la serie original (pot_weekly_tsbl) para evaluar visualmente el ajuste.

```{r}
h_future <- 52   # un a√±o

#pron√≥stico
fc_prophet <- fit_prophet %>%
  forecast(h = h_future)

#gr√°fico del pron√≥stico
autoplot(fc_prophet, pot_weekly_tsbl) +
  labs(title = "Pron√≥stico Prophet (52 semanas)",
       y = "Precio promedio semanal",
       x = "Tiempo") +
  theme_minimal()

head(fc_prophet, 10)

```

## **Extracci√≥n de componentes del modelo**

Extraemos y visualizamos los componentes internos que el modelo Prophet ha estimado: la tendencia (trend), la estacionalidad anual (season_year) y la estacionalidad semanal (season_week).

```{r}

components_prophet <- components(fit_prophet)

autoplot(components_prophet) +
  labs(title = "Componentes del modelo Prophet",
       x = "Tiempo", y = "") +
  theme_minimal()

```

## **Comparaci√≥n de m√©tricas**

Extraemos el pron√≥stico de Prophet, lo convertimos a un objeto ts (compatible con forecast::accuracy) y calculamos el RMSE, MAE y MAPE contra el set de prueba (y_test_w). Finalmente, lo comparamos con las m√©tricas ya calculadas de Holt-Winters (HW) y SARIMA.

```{r}


fc_prophet_mean <- fc_prophet %>%
  as_tibble() %>%
  pull(.mean)

length(fc_prophet_mean); length(y_test_w)

fc_prophet_mean_ts <- ts(
  fc_prophet_mean,
  start     = start(y_test_w),
  frequency = frequency(y_test_w)
)


acc_prophet <- forecast::accuracy(fc_prophet_mean_ts, y_test_w)


acc_hw <- forecast::accuracy(fc_hwm_w, y_test_w)


acc_arima <- forecast::accuracy(fc_seas, y_test_w)

acc_prophet
acc_hw
acc_arima


row_p   <- nrow(acc_prophet)
row_hw  <- nrow(acc_hw)
row_ar  <- nrow(acc_arima)

acc_compare <- data.frame(
  model = c("Prophet", "HW_multiplicativo", "SARIMA_52"),
  RMSE  = c(acc_prophet[row_p, "RMSE"],
            acc_hw[row_hw,    "RMSE"],
            acc_arima[row_ar, "RMSE"]),
  MAE   = c(acc_prophet[row_p, "MAE"],
            acc_hw[row_hw,    "MAE"],
            acc_arima[row_ar, "MAE"]),
  MAPE  = c(acc_prophet[row_p, "MAPE"],
            acc_hw[row_hw,    "MAPE"],
            acc_arima[row_ar, "MAPE"])
)

acc_compare

```

*Comparaci√≥n de desempe√±o entre Prophet, Holt‚ÄìWinters y SARIMA*

A partir de la ventana de prueba de 52 semanas, se evalu√≥ la capacidad predictiva de los tres modelos (Prophet, Holt-Winters multiplicativo y SARIMA(1,0,0)(0,1,0)[52]) utilizando las m√©tricas RMSE, MAE y MAPE.

Los resultados muestran diferencias claras en el desempe√±o:

* **Holt-Winters Multiplicativo (Ganador):** Obtuvo el MAPE m√°s bajo (19.58%) y el menor MAE (13.49). Esto indica que es el modelo con menor error porcentual fuera de muestra y la mejor estabilidad en la magnitud absoluta de sus errores.

* **Prophet:** Logr√≥ el RMSE m√°s bajo (16.27), sugiriendo que, en promedio, la desviaci√≥n de sus pron√≥sticos fue menor. Sin embargo, present√≥ el MAPE m√°s alto (34.91%), indicando errores proporcionalmente mayores en ciertos puntos, posiblemente por un sobreajuste o suavizado excesivo de las oscilaciones semanales del precio.

* **SARIMA(1,0,0)(0,1,0)[52]:** Mostr√≥ un RMSE similar a Holt-Winters (18.81), aunque su error porcentual (MAPE = 25.67%) fue superior. Esto sugiere que su captura de la estacionalidad (v√≠a diferencia estacional) no fue tan precisa como el patr√≥n multiplicativo de Holt-Winters.

## **Conclusi√≥n de la comparaci√≥n:**

El mejor equilibrio entre precisi√≥n (RMSE) y estabilidad (MAPE/MAE) fue logrado por el modelo Holt-Winters multiplicativo. No obstante, Prophet demostr√≥ ser competitivo en RMSE, y SARIMA se mantuvo como una alternativa s√≥lida intermedia.

## **Pruebas de estacionariedad Dickey Fuller**

Evaluamos la serie en tres estados: en niveles (original), con transformaci√≥n logar√≠tmica y con una primera diferencia sobre el logaritmo.

```{r}

library(tseries)

# Serie original semanal
y0 <- y_w 

#ADF test en niveles
adf_nivel <- adf.test(y0)
adf_nivel$p.value

#Transformaci√≥n log
y_log <- log(y0)
adf_log <- adf.test(y_log)
adf_log$p.value

#Diferencia de primer orden
y_diff <- diff(y_log, differences = 1)
adf_diff <- adf.test(na.omit(y_diff))
adf_diff$p.value

data.frame(
  Serie = c("Nivel", "Log", "Log diferencia 1"),
  p_value = c(adf_nivel$p.value, adf_log$p.value, adf_diff$p.value)
)

```
*Justificaci√≥n del uso de la serie como regresi√≥n*

De acuerdo con la teor√≠a, una regresi√≥n en series de tiempo requiere que las variables sean estacionarias para evitar regresiones espurias, donde se encuentran relaciones estad√≠sticas que no existen en la realidad. Por ello, se evalu√≥ la estacionariedad del precio semanal del Potato Red (la serie y_w).

**Resultados de la Prueba ADF (Augmented Dickey-Fuller)**

* Serie en Niveles: p-valor = 0.01

* Serie en Logaritmo: p-valor = 0.01

* Serie en Log-Diferencia (d=1): p-valor = 0.01

**Conclusi√≥n de Estacionariedad**
En los tres casos, el p-valor (0.01) es significativamente menor al nivel de significancia del 5% (0.05), lo que permite rechazar la hip√≥tesis nula (H0) de ra√≠z unitaria.

Esto confirma que la serie semanal es estacionaria (o al menos estacionaria en tendencia) en sus diferentes transformaciones. Este cumplimiento de la condici√≥n de estacionariedad valida el uso de enfoques de regresi√≥n para modelar la serie.

Tanto SARIMA (que puede verse como una regresi√≥n con errores ARMA) como Prophet (que se formula como un modelo de regresi√≥n no lineal aditivo) son, por lo tanto, enfoques te√≥ricamente viables para esta serie de tiempo.
